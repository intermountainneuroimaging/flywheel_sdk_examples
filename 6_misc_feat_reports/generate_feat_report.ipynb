{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEAT GROUP MODEL -- MULTIPLE COMPARISON CORRECTION AND REPORTING SCRIPT\n",
    "\n",
    "**Date modified:** 10/25/2024<br>\n",
    "**Authors:** Amy Hegarty, Intermountain Neuroimaging Consortium\n",
    "\n",
    "**Description:** <br>\n",
    "Work in prgress...\n",
    "User should update files paths in first section below before proceeding. \n",
    "\n",
    "Sections:\n",
    "1. **flywheel resolver** -- updated paths to local pl locations (skip if models were run outside flywheel)\n",
    "2. **randomise_gfeat_grid3** -- must up to date randomise script from banich lab. Computes permutation based statistics and inital cluster correction thresholding\n",
    "3. **randclustercontrasts** -- secondary scripts to store thresholded tstat maps, apply fdr, and generate cluster metrics\n",
    "4. **mine4stats and overlays** - generates \"*.summary\" folders used for visualization\n",
    "5. **pdf reports** (uncorrected zstats, fwe / fdr corrected tstats)\n",
    "6. **upload to flywheel**\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting...\n",
    "1. Be sure you have configured your conda environment to view ics managed conda environments and packages. If you haven't get started [here](https://inc-documentation.readthedocs.io/en/latest/pl_and_blanca_basics.html#setting-up-conda-environments).\n",
    "\n",
    "2. Be sure to select the `incenv` kernel from the list of availible kernels. If you don't see the `incenv` kernel, contact Amy Hegarty <Amy.Hegarty@colorado.edu> of follow the instructions [here](https://inc-documentation.readthedocs.io/en/latest/pl_and_blanca_basics.html#setting-up-conda-environments) to setup a new kernel in a shared conda environment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP PATHS\n",
    "Edit this section !!\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/path/to/group/level/feat/models\"    # path points to local folder location for gfeat models\n",
    "gfeat_prefix=\"\"                            # string to select which gfeat models to include... (useful in 3rd Level designs)\n",
    "flywheel_path=\"group/project\"              # Flywheel Group and Project (used for upload at the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import subprocess as sp\n",
    "import logging\n",
    "import time\n",
    "from string import Template\n",
    "\n",
    "# set default permissions\n",
    "os.umask(0o002);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shell(cmd, workdir=None):\n",
    "    terminal = sp.Popen(\n",
    "            cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE, universal_newlines=True, cwd=workdir\n",
    "        )\n",
    "    stdout, stderr = terminal.communicate()\n",
    "    log.debug(\"\\n %s\", stdout)\n",
    "    log.debug(\"\\n %s\", stderr)\n",
    "\n",
    "    output = stdout.strip(\"\\n\").split(\"\\n\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "log = logging.getLogger('main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def display_table(data):\n",
    "    html = \"<table>\"\n",
    "    for row in data:\n",
    "        html += \"<tr>\"\n",
    "        for field in row:\n",
    "            html += \"<td><h4>%s</h4></td>\"%(field)\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n",
    "    \n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findstem(arr):\n",
    " \n",
    "    # Determine size of the array\n",
    "    n = len(arr)\n",
    " \n",
    "    # Take first word from array\n",
    "    # as reference\n",
    "    s = arr[0]\n",
    "    l = len(s)\n",
    " \n",
    "    res = \"\"\n",
    " \n",
    "    for i in range(l):\n",
    "        for j in range(i + 1, l + 1):\n",
    " \n",
    "            # generating all possible substrings\n",
    "            # of our reference string arr[0] i.e s\n",
    "            stem = s[i:j]\n",
    "            k = 1\n",
    "            for k in range(1, n):\n",
    " \n",
    "                # Check if the generated stem is\n",
    "                # common to all words\n",
    "                if stem not in arr[k]:\n",
    "                    break\n",
    " \n",
    "            # If current substring is present in\n",
    "            # all strings and its length is greater\n",
    "            # than current result\n",
    "            if (k + 1 == n and len(res) < len(stem)):\n",
    "                res = stem\n",
    " \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_folder():\n",
    "    return os.path.dirname(os.path.realpath(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_compute_vars():\n",
    "    alpine_queue={\"qos\":\"normal\",\"partition\":\"amilan\",\"account\":\"ucb-general\"}\n",
    "    blanca_queue={\"qos\":\"blanca-ics\",\"partition\":\"blanca-ics\",\"account\":\"blanca-ics\"}\n",
    "    \n",
    "    if \"bnode\" in os.environ[\"HOSTNAME\"]:\n",
    "        return blanca_queue\n",
    "    else:\n",
    "        return alpine_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files_by_wildcard(pattern):\n",
    "    \"\"\"Removes files matching the given wildcard pattern.\"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    for file_path in files:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed: {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: RESOLVE FLYWHEEL PATHS\n",
    "If lower / intermediate models were run in flywheel, first resolve the lower level model paths. These needs to be run only once, but will allow mine4stats to correctly identify the lower level contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update this!!\n",
    "subject_feat_path = \"/path/to/your/subjectlevel/featmodels/\"\n",
    "\n",
    "#change path to be automated from above...\n",
    "cmd=\"\"\"#!/bin/bash\n",
    "\n",
    "umask g+w\n",
    "\n",
    "export PATH=$PWD/bin:$PATH\n",
    "# replace the filepath in lower level models so that fsfinfo works locally on pl!\n",
    "for FSFDIR1 in `ls -d {FEATPATH}/sub-*/ses-*/*.gfeat/cope*.feat` ; do\n",
    "\tfor MYVAR in `fsfinfo -i $FSFDIR1`; do\n",
    "\t\tif [[ $MYVAR == *\"gear-temp\"* ]]; then\n",
    "\t\t\tMYVARNEW=\"{FEATPATH}`echo $MYVAR | cut -d\"/\" -f12-`\"\n",
    "\t\t\techo \"Replacing: $MYVAR --> $MYVARNEW\"\n",
    "\t\t\tsed -i \"s#$MYVAR#$MYVARNEW#g\" \"$FSFDIR1/design.fsf\"\n",
    "\t\tfi\n",
    "\tdone\n",
    "done\n",
    "\"\"\"\n",
    "# writing batch file\n",
    "with open(\"rm.rename_paths.sh\",\"w+\") as f:\n",
    "    f.writelines(\n",
    "        cmd.format(**{\n",
    "           'FEATPATH': subject_feat_path,\n",
    "        })\n",
    "    )\n",
    "    \n",
    "shell(\". ./rm.rename_paths.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING!! REMOVES FILES BY WILDCARD. BE SURE BEFORE RUNNING!\n",
    "remove_files_by_wildcard(\"rm.*.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: RADOMISE CLUSTER CORRECTION\n",
    "Nonparametric permutation testing (fsl randomise) is used to estimate the statistics maps. Maps are thresholded using uncorrected and FWE corrected t-statistics. Cluster-wise correction was applied using either extent (e) or mass (m, recommended) approaches with  t>2.81 cluster threshold. Clusters were then thresholded at p=0.05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd=\"\"\"#!/bin/bash\n",
    "\n",
    "module use /projects/ics/modules\n",
    "module load fsl/6.0.3\n",
    "\n",
    "umask g+w\n",
    "\n",
    "export PATH={PATH}/bin:$PATH\n",
    "cd `dirname {FEATPATH}`\n",
    "\n",
    "# get gfeat folder for analysis\n",
    "gfeat=`basename {FEATPATH}`\n",
    "\n",
    "# to be sure all gfeat models ran to completion\n",
    "echo $gfeat\n",
    "featcheck $gfeat\n",
    "\n",
    "# move mask to working directory \n",
    "cp -p /pl/active/banich/masks/wager_masks/bin/bin_wager_gm_mask.nii.gz .\n",
    "\n",
    "######################################################################\n",
    "# RUN RANDOMISE\n",
    "######################################################################\n",
    "export FSL_SLURM_NUM_CPU=4\n",
    "export FSL_SLURM_UCB_ACCOUNT={ACCOUNT}\n",
    "export FSL_SLURM_PARTITION_NAME={PARTITION}\n",
    "export FSL_SLURM_QUEUE_NAME={QOS}\n",
    "export FSL_SLURM_WALLTIME_MINUTES=1440\n",
    "export FSL_SLURM_MB_RAM=16G\n",
    "\n",
    ". ${{FSLDIR}}/etc/fslconf/fsl.sh\n",
    "\n",
    "mask=$PWD/bin_wager_gm_mask.nii.gz\n",
    "cval=2.807034 # for p 0.0025 one-tail\n",
    "randomise_gfeat_grid3 $gfeat --uncorrp -n 10000 -m $mask -c $cval -C $cval -x\n",
    "\n",
    "echo Done!\n",
    "\"\"\"\n",
    "\n",
    "# use template above to run sbatch command to generate mine4stats results\n",
    "fullpath=os.path.join(path,gfeat_prefix+\"*.gfeat\")\n",
    "gfeatdirs = glob.glob(fullpath)\n",
    "hpc=set_compute_vars()\n",
    "\n",
    "# writing batch file\n",
    "for idx,gfeat in enumerate(gfeatdirs):\n",
    "    with open(\"rm.randomise\"+str(idx)+\".sh\",\"w+\") as f:\n",
    "        f.writelines(\n",
    "            cmd.format(**{\n",
    "               'PATH':os.getcwd(),  \n",
    "               'FEATPATH': gfeat,\n",
    "               'QOS': hpc[\"qos\"], \n",
    "               'PARTITION': hpc[\"partition\"], \n",
    "               'ACCOUNT': hpc[\"account\"]\n",
    "            })\n",
    "        )\n",
    "\n",
    "    # running batch file\n",
    "    shell(\". ./rm.randomise\"+str(idx)+\".sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!Wait for all jobs to finish on the queue!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if job has finished on queue... if no jobs are showing...proceed.\n",
    "shell('squeue -u $USER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING!! REMOVES FILES BY WILDCARD. BE SURE BEFORE RUNNING!\n",
    "remove_files_by_wildcard(\"rm.*.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: RANDOMISE CLUSTER REPORTS\n",
    "Clusters, corrected for multiple comparisons using either FDR, FWE, voxel, and uncorrected, are recorded. Cluster centers are matched based on binary atlas mapping to the Talairach and Harvard Oxford (Cortical and Subcortical) Structral atalases. Local maxima are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd=\"\"\"#!/bin/bash\n",
    "#\n",
    "#SBATCH --job-name=randomise.clusters\n",
    "#SBATCH --qos={QOS}\n",
    "#SBATCH --partition={PARTITION}\n",
    "#SBATCH --account={ACCOUNT}\n",
    "#SBATCH --time=24:00:00\n",
    "#SBATCH --array=1-{NDIRS}\n",
    "#SBATCH --output=logs/randomise_clusters_%A_%a.out\n",
    "#SBATCH --error=logs/randomise_clusters_%A_%a.err\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem=16G\n",
    "\n",
    "umask g+w\n",
    "\n",
    "module use /projects/ics/modules\n",
    "module load fsl/6.0.3\n",
    "\n",
    "export PATH={PATH}/bin:$PATH\n",
    "cd {FEATPATH}\n",
    "\n",
    "# get gfeat folder for analysis\n",
    "gfeat=`ls -d {PREFIX}*.gfeat | sed -n \"$SLURM_ARRAY_TASK_ID p\"`\n",
    "\n",
    "######################################################################\n",
    "# For FDR, specify GM mask instead of default cope*.feat/mask.nii.gz\n",
    "# with output in *.gfeat/fdrmask_summary_stats\n",
    "######################################################################\n",
    "#     fdrmask       = 0.95\n",
    "#     fwepclustrerm = 0.95\n",
    "#     fwepvox       = 0.95\n",
    "#     vox_uncorrp   = 0.999\n",
    "#\n",
    "# ========= RUN ANY OF THESE  =========\n",
    "# ......... FDR\n",
    "randclustercontrasts10000_fdrmask $PWD/$gfeat $mask\n",
    "\n",
    "# ......... FWEP CLUSTERM\n",
    "randclustercontrasts10000_fwep_clusterm $PWD/$gfeat\n",
    "\n",
    "# ......... FWEP VOX\n",
    "randclustercontrasts10000_fwep_vox $PWD/$gfeat\n",
    "\n",
    "# ......... VOX UNCORRP (vox_uncorrp = 0.999)\n",
    "randclustercontrasts10000_vox_uncorrp $PWD/$gfeat\n",
    "\n",
    "\n",
    "# After\n",
    "#    (a) randomise has completed\n",
    "#    (b) cluster reports are run\n",
    "# with outputs in*.gfeat/*summary_stats/thresh_tstats\n",
    "\n",
    "# Run jobs on the grid since the Tal Daemon calls have been replaced\n",
    "# by references to the FSL Tal Atlas\n",
    "\n",
    "# -- generate *.gfeat/*summary_stats/localmax based on ../thresh_tstats\n",
    "# randomise clusters\n",
    "# ========= RUN ANY OF THESE  =========\n",
    "# ......... FDR\n",
    "peakdist=12\n",
    "n=50\n",
    "s=$gfeat/fdrmask_summary_stats\n",
    "echo $s\n",
    "getlocalmax4 $s $peakdist $n\n",
    "\n",
    "#\n",
    "#  ......... FWEP CLUSTERM\n",
    "peakdist=12\n",
    "n=50\n",
    "s=$gfeat/fwepclusterm_summary_stats\n",
    "echo $s\n",
    "getlocalmax4 $s $peakdist $n\n",
    "\n",
    "#\n",
    "#  ......... FWEP VOX\n",
    "peakdist=12\n",
    "n=50\n",
    "s=$gfeat/fwepvox_summary_stats\n",
    "echo $s\n",
    "getlocalmax4 $s $peakdist $n\n",
    "\n",
    "#\n",
    "#  ......... VOX UNCORRP\n",
    "peakdist=12\n",
    "n=50\n",
    "s=$gfeat/vox_uncorrp_summary_stats\n",
    "echo $s\n",
    "getlocalmax4 $s $peakdist $n\n",
    "\n",
    "\n",
    "echo Done!\n",
    "\"\"\"\n",
    "\n",
    "# use template above to run sbatch command to generate mine4stats results\n",
    "fullpath=os.path.join(path,gfeat_prefix+\"*.gfeat\")\n",
    "gfeatdirs = glob.glob(fullpath)\n",
    "hpc=set_compute_vars()\n",
    "\n",
    "# writing batch file\n",
    "with open(\"rm.randomise.clusters.sh\",\"w+\") as f:\n",
    "    f.writelines(\n",
    "        cmd.format(**{\n",
    "           'PATH':os.getcwd(), \n",
    "           'NDIRS':len(gfeatdirs), \n",
    "           'FEATPATH': path,\n",
    "           'PREFIX': gfeat_prefix, \n",
    "           'QOS': hpc[\"qos\"], \n",
    "           'PARTITION': hpc[\"partition\"], \n",
    "           'ACCOUNT': hpc[\"account\"]\n",
    "        })\n",
    "    )\n",
    "\n",
    "# running batch file\n",
    "shell(\"sbatch rm.randomise.clusters.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if job has finished on queue... if no jobs are showing...proceed.\n",
    "shell('squeue -u $USER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!Wait for jobs to finish on the cluster!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING!! REMOVES FILES BY WILDCARD. BE SURE BEFORE RUNNING!\n",
    "remove_files_by_wildcard(\"rm.*.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: MINE4STATS + OVERLAYS\n",
    "Univariate model results are stored in a summary folder for ease of review. Uncorrected zstats, and if available randomise tstats thresholded using FDR, FWE, TFCE methods. A description of the contrast names and lower model design are also stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd=\"\"\"#!/bin/bash\n",
    "#\n",
    "#SBATCH --job-name=mine4stats\n",
    "#SBATCH --qos={QOS}\n",
    "#SBATCH --partition={PARTITION}\n",
    "#SBATCH --account={ACCOUNT}\n",
    "#SBATCH --time=00:30:00\n",
    "#SBATCH --array=1-{NDIRS}\n",
    "#SBATCH --output=logs/mine4stats_%A_%a.out\n",
    "#SBATCH --error=logs/mine4stats_%A_%a.err\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --mem=8G\n",
    "\n",
    "module use /projects/ics/modules\n",
    "module load fsl/6.0.7\n",
    "\n",
    "export PATH={PATH}/bin:$PATH\n",
    "cd {FEATPATH}\n",
    "gfeat=`ls -d {PREFIX}*.gfeat | sed -n \"$SLURM_ARRAY_TASK_ID p\"`\n",
    "mine4stats_randomise $gfeat -z -x -a -d -r ;\n",
    "\n",
    "# generate LOWER_DESIGN spreadsheet\n",
    "sumdir=$(echo $gfeat | sed 's|.gfeat|.summary|')\n",
    "fsfcheck $sumdir/lower_design.fsf | egrep '(EV|contrast)' | grep -v IRR > $sumdir/LOWER_DESIGN.txt\n",
    "\n",
    "# overlay - produce static images\n",
    "getoverlays $sumdir\n",
    "\n",
    "echo Done!\n",
    "\"\"\"\n",
    "\n",
    "# use template above to run sbatch command to generate mine4stats results\n",
    "fullpath=os.path.join(path,gfeat_prefix+\"*.gfeat\")\n",
    "gfeatdirs = glob.glob(fullpath)\n",
    "hpc=set_compute_vars()\n",
    "\n",
    "# writing batch file\n",
    "with open(\"rm.mine4stats.sh\",\"w+\") as f:\n",
    "    f.writelines(\n",
    "        cmd.format(**{\n",
    "           'PATH':os.getcwd(), \n",
    "           'NDIRS':len(gfeatdirs), \n",
    "           'FEATPATH': path,\n",
    "           'PREFIX': gfeat_prefix, \n",
    "           'QOS': hpc[\"qos\"], \n",
    "           'PARTITION': hpc[\"partition\"], \n",
    "           'ACCOUNT': hpc[\"account\"]\n",
    "        })\n",
    "    )\n",
    "\n",
    "# running batch file\n",
    "shell(\"sbatch rm.mine4stats.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if job has finished on queue... if no jobs are showing...proceed.\n",
    "shell('squeue -u $USER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!Wait for jobs to finish on the cluster!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING!! REMOVES FILES BY WILDCARD. BE SURE BEFORE RUNNING!\n",
    "remove_files_by_wildcard(\"rm.*.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: ZIP SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create zip\n",
    "os.makedirs(os.path.join(path,\"zips\"),exist_ok=True)\n",
    "cmd=\"zip -rq \"+os.path.join(\"zips\",gfeat_prefix+\"_\"+time.strftime(\"%Y%m%d\")+\".zip\")+\" \"+os.path.join(gfeat_prefix+\"*.summary/{*.png,*.fsf,LOWER_DESIGN.txt,README.txt,zstats,randomise,cluster_reports}\")\n",
    "# print(cmd)\n",
    "shell(cmd,workdir=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: CONTRAST PDF REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_copenum(paths):\n",
    "    try:\n",
    "        df = pd.DataFrame()\n",
    "        df[\"path\"] = sumdirs\n",
    "        df[\"copnum\"] = df[\"path\"].str.extract(r'cope(\\d+)').astype(int)\n",
    "        df = df.sort_values(\"copnum\")\n",
    "    except:\n",
    "        pass\n",
    "    return df[\"path\"].values\n",
    "\n",
    "def split_list_to_dict(lst):\n",
    "    result_dict = {}\n",
    "    for item in lst:\n",
    "        key, value = item.split(' ', 1)  # Split on first space\n",
    "        result_dict[key] = value\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add subject specific plots (snr, motion, carpet)\n",
    "sumdirs = glob.glob(os.path.join(path,gfeat_prefix+\"*.summary\"))\n",
    "sumdirs_sorted = sort_by_copenum(sumdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumdirs_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish lower, intermediate (if applicable), and higher level\n",
    "# put contrast names from lower and higher levels into an array\n",
    "cmd=\"\"\"#!/bin/bash\n",
    "module use /projects/ics/modules\n",
    "module load fsl/6.0.7\n",
    "export PATH=$PWD/bin:$PATH\n",
    "SOURCE={SOURCE}\n",
    "lname=(\"\" `fsfinfo -lcon $SOURCE | sed 's|contrast [0-9 ]* ||' | sed -e 's|[() ]|_|g' -e 's|>|-|g' -e 's|<|-lt-|g' -e 's|&|+|g'`); \\\n",
    "hname=(\"\" `fsfinfo -hcon $SOURCE | sed 's|contrast [0-9 ]* ||' | sed -e 's|[() ]|_|g' -e 's|>|-|g' -e 's|<|-lt-|g' -e 's|&|+|g'`)\n",
    "\n",
    "fsfinfo -i $SOURCE | head -1 | grep -q '\\.gfeat/'; if [ $? -eq 0 ]; then  ### SOURCE IS 3-LEVEL ANALYSIS\n",
    "hasIntermediate=set\n",
    "iname=(\"\" `fsfinfo -icon $SOURCE | sed 's|contrast [0-9 ]* ||' | sed -e 's|[() ]|_|g' -e 's|>|-|g' -e 's|<|-lt-|g' -e 's|&|+|'`)\n",
    "fi\n",
    "\n",
    "### ASK PERMISSION TO CONTINUE\n",
    "echo \"LOWER LEVEL CONTRASTS:\";  n=1; for i in ${{lname[*]}}; do echo $n $i; let n++; done\n",
    "[ $hasIntermediate ] && echo \"INTERMEDIATE LEVEL CONTRASTS:\";  n=1; for i in ${{iname[*]}}; do echo $n $i; let n++; done\n",
    "echo \"HIGHER LEVEL CONTRASTS:\"; n=1; for i in ${{hname[*]}}; do echo $n $i; let n++; done\n",
    "\"\"\"\n",
    "\n",
    "# writing batch file\n",
    "with open(\"rm.get_design.sh\",\"w+\") as f:\n",
    "    f.writelines(\n",
    "        cmd.format(**{'SOURCE':os.path.join(sumdirs_sorted[0].replace(\".summary\",\".gfeat\"))})\n",
    "    )\n",
    "out = shell(\". ./rm.get_design.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING!! REMOVES FILES BY WILDCARD. BE SURE BEFORE RUNNING!\n",
    "remove_files_by_wildcard(\"rm.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull cope name and cope number mapping for all levels\n",
    "lid=out.index(\"LOWER LEVEL CONTRASTS:\")\n",
    "hid=out.index(\"HIGHER LEVEL CONTRASTS:\")\n",
    "\n",
    "if \"INTERMEDIATE LEVEL CONTRASTS:\" in out:\n",
    "    iid=out.index(\"INTERMEDIATE LEVEL CONTRASTS:\")\n",
    "    lower = split_list_to_dict(out[lid+1:iid])\n",
    "    inter = split_list_to_dict(out[iid+1:hid])\n",
    "    higher = split_list_to_dict(out[hid+1:])\n",
    "else:\n",
    "    lower = split_list_to_dict(out[lid+1:hid])\n",
    "    higher = split_list_to_dict(out[hid+1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.enums import TA_JUSTIFY\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.pagesizes import letter, landscape\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak, NextPageTemplate, PageTemplate, Frame, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.rl_config import defaultPageSize\n",
    "from reportlab.lib import utils\n",
    "\n",
    "# initialize report settings\n",
    "PAGE_HEIGHT=defaultPageSize[1]\n",
    "styles = getSampleStyleSheet()\n",
    "Elements=[]\n",
    "HeaderStyle = styles[\"Heading1\"]\n",
    "HeaderStyle.alignment = 1\n",
    "ParaStyle = styles[\"Normal\"]\n",
    "PreStyle = styles[\"Code\"]\n",
    "\n",
    "def header(txt, style=HeaderStyle, klass=Paragraph, sep=0.3):\n",
    "    s = Spacer(0.2*inch, sep*inch)\n",
    "    Elements.append(s)\n",
    "    para = klass(txt, style)\n",
    "    Elements.append(para)\n",
    "\n",
    "    \n",
    "def p(txt):\n",
    "    return header(txt, style=ParaStyle, sep=0.1)\n",
    "\n",
    "\n",
    "def make_portrait(canvas,doc):\n",
    "    canvas.setPageSize(letter)\n",
    "\n",
    "    \n",
    "def make_landscape(canvas,doc):\n",
    "    canvas.setPageSize(landscape(letter))\n",
    "    \n",
    "    \n",
    "def build_report():\n",
    "    doc = SimpleDocTemplate(reportfile,pagesize=letter,\n",
    "                        rightMargin=72,leftMargin=72,\n",
    "                        topMargin=72,bottomMargin=18)\n",
    "    \n",
    "    frame1 = Frame(doc.leftMargin, doc.topMargin,\n",
    "                doc.width, doc.height,\n",
    "                leftPadding = 0, rightPadding = 0,\n",
    "                topPadding = 0, bottomPadding = 0,\n",
    "                id='frame1')\n",
    "    \n",
    "    frame2 = Frame(doc.leftMargin, doc.topMargin,\n",
    "                doc.height, doc.width,\n",
    "                leftPadding = 0, rightPadding = 0,\n",
    "                topPadding = 0, bottomPadding = 0,\n",
    "                id='frame1')\n",
    "    ptemplate = PageTemplate(id='portrait',frames =[frame1], onPage=make_portrait)\n",
    "    ltemplate = PageTemplate(id='landscape',frames =[frame2], onPage=make_landscape)\n",
    "    \n",
    "    doc.addPageTemplates([ptemplate, ltemplate])\n",
    "    \n",
    "    doc.build(Elements)\n",
    "\n",
    "\n",
    "def get_image(path, width=1*inch):\n",
    "    img = utils.ImageReader(path)\n",
    "    iw, ih = img.getSize()\n",
    "    aspect = ih / float(iw)\n",
    "    im = Image(path, width=width, height=(width * aspect))\n",
    "    im.hAlign = 'LEFT'\n",
    "    return im\n",
    "\n",
    "    \n",
    "def add_table(outliers):\n",
    "    # add table of \"outlier values\"\n",
    "    outliers['y'] = outliers['y'].apply(lambda x: round(x, 2))\n",
    "    t=Table(outliers[[\"subject\", \"session\",\"acq\",\"y\"]].values.tolist())\n",
    "    Elements.append(t)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNCORRECTED ZSTAT REPORT (with INTERMEDIATE COPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_line1 = \"Intermountain Neuroimaging Consortium\".upper()\n",
    "Title_line2 = \"UNIVARIATE ANALYSIS REPORT\".upper()\n",
    "TodayDate = date.today().strftime(\"%B %d, %Y\")\n",
    "Author = \"Created By: \"+os.environ[\"USER\"]\n",
    "url = \"<github link>\"\n",
    "ProjectName = gfeat_prefix\n",
    "reportfile = os.path.join(path,\"zips\",gfeat_prefix+\"_zstats_\"+time.strftime(\"%Y%m%d\")+\".pdf\")\n",
    "\n",
    "header(Title_line1)\n",
    "header(Title_line2,sep=0.05)\n",
    "header(ProjectName, sep=0.05, style=styles[\"Heading2\"])\n",
    "\n",
    "header(TodayDate, sep=0.1, style=ParaStyle)\n",
    "header(Author, sep=0.1, style=ParaStyle)\n",
    "header(url, sep=0.1, style=ParaStyle)\n",
    "counter=0\n",
    "\n",
    "p(\"This work utilized the CUmulus on-premise cloud service at the University of Colorado Boulder. CUmulus is jointly \"\n",
    "  \"funded by the National Science Foundation (award OAC-1925766) and the University of Colorado Boulder.\") \n",
    "\n",
    "p(\"Data storage supported by the University of Colorado Boulder 'PetaLibrary'\")\n",
    "\n",
    "# Elements.append(NextPageTemplate('landscape'))\n",
    "Elements.append(PageBreak())\n",
    "\n",
    "header(\"UNCORRECTED ZSTATS\", style=styles[\"Heading3\"])\n",
    "\n",
    "for d in sumdirs_sorted:\n",
    "    if 'inter' in locals():\n",
    "        copenum = re.search('cope(\\d+)',d).group(1)\n",
    "        copename = lower[copenum]\n",
    "\n",
    "        header(\"LOWER LEVEL CONTRAST: \"+copenum+\": \"+copename.replace(\"_\",\" \").replace(\"gt\",\">\"), style=styles[\"Heading4\"])\n",
    "    \n",
    "    chart_style = TableStyle([('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "                                  ('VALIGN', (0, 0), (-1, -1), 'MIDDLE')])\n",
    "\n",
    "    image1 = get_image(os.path.join(d, \"overlays/render1/ramp2.gif\"), width=150)\n",
    "    image2 = get_image(os.path.join(d, \"overlays/render1/ramp.gif\"), width=150)\n",
    "\n",
    "    Elements.append(Table([[\"zstat: -5                             -2.58\",\"  2.58                                       5\"],[image1, image2]],\n",
    "                         colWidths=[2 * inch, 2 * inch],\n",
    "                         rowHeights=[0.15 * inch, 0.4 * inch], style=chart_style))\n",
    "    \n",
    "    # setup expected naming scheme\n",
    "    for h in higher.keys():\n",
    "        header(\"HIGHER LEVEL CONTRAST: \"+higher[h], style=styles[\"Heading5\"])\n",
    "        if 'inter' in locals():\n",
    "            for i in inter.keys():\n",
    "                # find corresponding z-stat image from overlays\n",
    "                imgpaths = glob.glob(os.path.join(d,\"overlays/render1/slices1_\"+copename+\"_\"+inter[i]+\"_\"+higher[h]+\".png\"))\n",
    "                log.debug(higher[h]+\"... cope\"+str(i)+\": \"+inter[i].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                log.debug(\"file: %s\", imgpaths[0])\n",
    "                if imgpaths:\n",
    "                    p(higher[h]+\"... cope\"+str(i)+\": \"+inter[i].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                    Elements.append(get_image(imgpaths[0], width=500))\n",
    "        else:\n",
    "            for l in lower.keys():\n",
    "                # find corresponding z-stat image from overlays\n",
    "                imgpaths = glob.glob(os.path.join(d,\"overlays/render1/slices1_\"+lower[l]+\"_\"+higher[h]+\".png\"))\n",
    "                log.debug(higher[h]+\"... cope\"+str(l)+\": \"+lower[l].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                log.debug(\"file: %s\", imgpaths[0])\n",
    "                if imgpaths:\n",
    "                    p(higher[h]+\"... cope\"+str(l)+\": \"+lower[l].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                    Elements.append(get_image(imgpaths[0], width=500))\n",
    "    \n",
    "    Elements.append(PageBreak())\n",
    "\n",
    "build_report()\n",
    "log.info(\"Report written: %s\", reportfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOMISE CLUSTER CORRECTED REPORT: FWE CLUSTERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_line1 = \"Intermountain Neuroimaging Consortium\".upper()\n",
    "Title_line2 = \"UNIVARIATE ANALYSIS REPORT\".upper()\n",
    "TodayDate = date.today().strftime(\"%B %d, %Y\")\n",
    "Author = \"Created By: \"+os.environ[\"USER\"]\n",
    "url = \"<github link>\"\n",
    "ProjectName = gfeat_prefix\n",
    "reportfile = os.path.join(path,\"zips\",gfeat_prefix+\"_fwep_clusterm_tstats_\"+time.strftime(\"%Y%m%d\")+\".pdf\")\n",
    "\n",
    "header(Title_line1)\n",
    "header(Title_line2,sep=0.05)\n",
    "header(ProjectName, sep=0.05, style=styles[\"Heading2\"])\n",
    "\n",
    "header(TodayDate, sep=0.1, style=ParaStyle)\n",
    "header(Author, sep=0.1, style=ParaStyle)\n",
    "header(url, sep=0.1, style=ParaStyle)\n",
    "counter=0\n",
    "\n",
    "p(\"This work utilized the CUmulus on-premise cloud service at the University of Colorado Boulder. CUmulus is jointly \"\n",
    "  \"funded by the National Science Foundation (award OAC-1925766) and the University of Colorado Boulder.\") \n",
    "\n",
    "p(\"Data storage supported by the University of Colorado Boulder 'PetaLibrary'\")\n",
    "\n",
    "# Elements.append(NextPageTemplate('landscape'))\n",
    "Elements.append(PageBreak())\n",
    "\n",
    "header(\"RANDOMISE: FWE CLUSTER MASS CORRECTED TSTATS\", style=styles[\"Heading3\"])\n",
    "\n",
    "for d in sumdirs_sorted:\n",
    "    # CLUSTER CORRECTED\n",
    "    if os.path.exists(os.path.join(d,\"randomise\")):\n",
    "        \n",
    "        if 'inter' in locals():\n",
    "            copenum = re.search('cope(\\d+)',d).group(1)\n",
    "            copename = lower[copenum]\n",
    "            header(\"LOWER LEVEL CONTRAST: \"+copenum+\": \"+copename.replace(\"_\",\" \").replace(\"gt\",\">\"), style=styles[\"Heading4\"])\n",
    "\n",
    "        chart_style = TableStyle([('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "                                      ('VALIGN', (0, 0), (-1, -1), 'MIDDLE')])\n",
    "\n",
    "        image1 = get_image(os.path.join(d, \"overlays/render1/ramp2.gif\"), width=150)\n",
    "        image2 = get_image(os.path.join(d, \"overlays/render1/ramp.gif\"), width=150)\n",
    "\n",
    "        Elements.append(Table([[\"tstat: -5                             -2.58\",\"  2.58                                       5\"],[image1, image2]],\n",
    "                             colWidths=[2 * inch, 2 * inch],\n",
    "                             rowHeights=[0.15 * inch, 0.4 * inch], style=chart_style))\n",
    "        \n",
    "        # setup expected naming scheme\n",
    "        for h in higher.keys():\n",
    "            header(\"HIGHER LEVEL CONTRAST: \"+higher[h], style=styles[\"Heading5\"])\n",
    "            # analyses with intermediate results\n",
    "            if 'inter' in locals():\n",
    "                for i in inter.keys():\n",
    "                    # find corresponding z-stat image from overlays\n",
    "                    imgpaths = glob.glob(os.path.join(d,\"overlays/render1/slices1_\"+copename+\"_thresh_fwep_clusterm_tstat\"+str(h)+\"_\"+inter[i]+\"*.png\"))\n",
    "\n",
    "                    if imgpaths:\n",
    "                        log.debug(higher[h]+\"... cope\"+str(i)+\": \"+inter[i].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                        log.debug(\"file: %s\", imgpaths[0])\n",
    "                        p(higher[h]+\"... cope\"+str(i)+\": \"+inter[i].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                        Elements.append(get_image(imgpaths[0], width=500))\n",
    "            else:\n",
    "                for l in lower.keys():\n",
    "                    # find corresponding z-stat image from overlays\n",
    "                    imgpaths = glob.glob(os.path.join(d,\"overlays/render1/slices1_thresh_fwep_clusterm_tstat\"+str(h)+\"_\"+lower[l]+\"*.png\"))\n",
    "\n",
    "                    if imgpaths:\n",
    "                        log.debug(higher[h]+\"... cope\"+str(l)+\": \"+lower[l].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                        log.debug(\"file: %s\", imgpaths[0])\n",
    "                        p(higher[h]+\"... cope\"+str(l)+\": \"+lower[l].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                        Elements.append(get_image(imgpaths[0], width=500))\n",
    "                \n",
    "        Elements.append(PageBreak())\n",
    "\n",
    "build_report()\n",
    "log.info(\"Report written: %s\", reportfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOMISE CLUSTER CORRECTED REPORT: FDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_line1 = \"Intermountain Neuroimaging Consortium\".upper()\n",
    "Title_line2 = \"UNIVARIATE ANALYSIS REPORT\".upper()\n",
    "TodayDate = date.today().strftime(\"%B %d, %Y\")\n",
    "Author = \"Created By: \"+os.environ[\"USER\"]\n",
    "url = \"<github link>\"\n",
    "ProjectName = gfeat_prefix\n",
    "reportfile = os.path.join(path,\"zips\",gfeat_prefix+\"_fdr_tstats_\"+time.strftime(\"%Y%m%d\")+\".pdf\")\n",
    "\n",
    "header(Title_line1)\n",
    "header(Title_line2,sep=0.05)\n",
    "header(ProjectName, sep=0.05, style=styles[\"Heading2\"])\n",
    "\n",
    "header(TodayDate, sep=0.1, style=ParaStyle)\n",
    "header(Author, sep=0.1, style=ParaStyle)\n",
    "header(url, sep=0.1, style=ParaStyle)\n",
    "counter=0\n",
    "\n",
    "p(\"This work utilized the CUmulus on-premise cloud service at the University of Colorado Boulder. CUmulus is jointly \"\n",
    "  \"funded by the National Science Foundation (award OAC-1925766) and the University of Colorado Boulder.\") \n",
    "\n",
    "p(\"Data storage supported by the University of Colorado Boulder 'PetaLibrary'\")\n",
    "\n",
    "# Elements.append(NextPageTemplate('landscape'))\n",
    "Elements.append(PageBreak())\n",
    "\n",
    "header(\"RANDOMISE: FDR` CORRECTED TSTATS\", style=styles[\"Heading3\"])\n",
    "\n",
    "for d in sumdirs_sorted:\n",
    "    # CLUSTER CORRECTED\n",
    "    if os.path.exists(os.path.join(d,\"randomise\")):\n",
    "        if 'inter' in locals():\n",
    "            copenum = re.search('cope(\\d+)',d).group(1)\n",
    "            copename = lower[copenum]\n",
    "            header(\"LOWER LEVEL CONTRAST: \"+copenum+\": \"+copename.replace(\"_\",\" \").replace(\"gt\",\">\"), style=styles[\"Heading4\"])\n",
    "\n",
    "        chart_style = TableStyle([('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "                                      ('VALIGN', (0, 0), (-1, -1), 'MIDDLE')])\n",
    "\n",
    "        image1 = get_image(os.path.join(d, \"overlays/render1/ramp2.gif\"), width=150)\n",
    "        image2 = get_image(os.path.join(d, \"overlays/render1/ramp.gif\"), width=150)\n",
    "\n",
    "        Elements.append(Table([[\"tstat: -5                             -2.58\",\"  2.58                                       5\"],[image1, image2]],\n",
    "                             colWidths=[2 * inch, 2 * inch],\n",
    "                             rowHeights=[0.15 * inch, 0.4 * inch], style=chart_style))\n",
    "        \n",
    "        # setup expected naming scheme\n",
    "        for h in higher.keys():\n",
    "            header(\"HIGHER LEVEL CONTRAST: \"+higher[h], style=styles[\"Heading5\"])\n",
    "            if 'inter' in locals():\n",
    "                for i in inter.keys():\n",
    "                    # find corresponding z-stat image from overlays\n",
    "                    imgpaths = glob.glob(os.path.join(d,\"overlays/render1/slices1_\"+copename+\"_thresh_fdr_tstat\"+str(h)+\"_\"+inter[i]+\"*.png\"))\n",
    "\n",
    "                    if imgpaths:\n",
    "                        log.debug(higher[h]+\"... cope\"+str(i)+\": \"+inter[i].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                        log.debug(\"file: %s\", imgpaths[0])\n",
    "                        p(higher[h]+\"... cope\"+str(i)+\": \"+inter[i].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                        Elements.append(get_image(imgpaths[0], width=500))\n",
    "            else:\n",
    "                for l in lower.keys():\n",
    "                    # find corresponding z-stat image from overlays\n",
    "                    imgpaths = glob.glob(os.path.join(d,\"overlays/render1/slices1_thresh_fdr_tstat\"+str(h)+\"_\"+lower[l]+\"*.png\"))\n",
    "\n",
    "                    if imgpaths:\n",
    "                        log.debug(higher[h]+\"... cope\"+str(l)+\": \"+lower[l].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                        log.debug(\"file: %s\", imgpaths[0])\n",
    "                        p(higher[h]+\"... cope\"+str(l)+\": \"+lower[l].replace(\"_\",\" \").replace(\"gt\",\">\"))\n",
    "                        Elements.append(get_image(imgpaths[0], width=500))\n",
    "        Elements.append(PageBreak())\n",
    "\n",
    "build_report()\n",
    "log.info(\"Report written: %s\", reportfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: UPLOAD SUMMARY TO FLYWHEEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using The Flywheel CLI or SDK for the first time?\n",
    "1. You need to use Flywheel's CLI login process to store your flywheel credentials in: `/home/$USER/.config/flywheel/user.json`\n",
    "2. Access the flywheel CLI using these [instructions](https://inc-documentation.readthedocs.io/en/latest/cli_basics.html#cli-from-blanca-compute-node)\n",
    "3. Generate an individualized API key using these [instructions](https://docs.flywheel.io/admin/api_keys/admin_creating_a_user_api_key/)\n",
    "\n",
    "Thats it! You should now be able to access your Flywheel projects!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flywheel\n",
    "fw = flywheel.Client('')\n",
    "\n",
    "def get_analysis_files(analysis):\n",
    "    names=[]\n",
    "    for f in analysis.files:\n",
    "        names.append(f.name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = fw.lookup(flywheel_path)\n",
    "project = fw.get_project(project.id)\n",
    "\n",
    "analysis_name = \"Univariate Analyses \"+time.strftime(\"%m/%d/%Y %H:%M\")\n",
    "if not fw.analyses.find('label='+analysis_name):\n",
    "    analysis = project.add_analysis(label=analysis_name)\n",
    "else:\n",
    "    analysis = fw.analyses.find_one('label='+analysis_name)\n",
    "\n",
    "files = glob.glob(os.path.join(path,\"zips\",gfeat_prefix+\"*\"))\n",
    "\n",
    "for f in files:\n",
    "    if os.path.basename(f) not in get_analysis_files(analysis):\n",
    "        analysis.upload_output(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incenv",
   "language": "python",
   "name": "incenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
